{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting sportypy\n",
      "  Downloading sportypy-1.0.0-py3-none-any.whl.metadata (21 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sportypy) (1.26.4)\n",
      "Requirement already satisfied: scipy in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sportypy) (1.14.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sportypy) (2.2.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sportypy) (3.9.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (4.53.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (24.1)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib->sportypy) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->sportypy) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->sportypy) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vijay\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->sportypy) (1.16.0)\n",
      "Downloading sportypy-1.0.0-py3-none-any.whl (163 kB)\n",
      "Installing collected packages: sportypy\n",
      "Successfully installed sportypy-1.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install sportypy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "MovieWriter imagemagick unavailable; using Pillow instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "frame: 180009.00\n",
      "len(frame_data): 1\n",
      "len(players), len(balls): 1 1\n",
      "frame: 180009.00\n",
      "len(frame_data): 1\n",
      "len(players), len(balls): 1 1\n",
      "frame: 180009.00\n",
      "len(frame_data): 1\n",
      "len(players), len(balls): 1 1\n",
      "frame: 180009.00\n",
      "len(frame_data): 1\n",
      "len(players), len(balls): 1 1\n",
      "frame: 180117.33\n",
      "len(frame_data): 6\n",
      "len(players), len(balls): 6 6\n",
      "frame: 180225.67\n",
      "len(frame_data): 10\n",
      "len(players), len(balls): 10 10\n",
      "frame: 180334.00\n",
      "len(frame_data): 13\n",
      "len(players), len(balls): 13 13\n",
      "frame: 180442.33\n",
      "len(frame_data): 17\n",
      "len(players), len(balls): 17 17\n",
      "frame: 180550.67\n",
      "len(frame_data): 21\n",
      "len(players), len(balls): 21 21\n",
      "frame: 180659.00\n",
      "len(frame_data): 28\n",
      "len(players), len(balls): 28 28\n",
      "frame: 180767.33\n",
      "len(frame_data): 38\n",
      "len(players), len(balls): 38 38\n",
      "frame: 180875.67\n",
      "len(frame_data): 46\n",
      "len(players), len(balls): 46 46\n",
      "frame: 180984.00\n",
      "len(frame_data): 50\n",
      "len(players), len(balls): 50 50\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sportypy.surfaces import MiLBField\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "import numpy as np\n",
    "\n",
    "# The dimensions are not exactly like this but this is an example if you need something to go off of\n",
    "\n",
    "num_rows = 50\n",
    "\n",
    "np.random.seed(0)  # it will always generate the same data - so it is simpler to compare them \n",
    "\n",
    "data = {\n",
    "    'game_str': ['game_01'] * num_rows,\n",
    "    'play_id': [10] * num_rows,\n",
    "    'timestamp': np.random.randint(180000, 181000, size=num_rows),\n",
    "    'player_position': np.random.randint(1, 11, size=num_rows),\n",
    "    'field_x': np.random.uniform(-150, 150, size=num_rows),\n",
    "    'field_y': np.random.uniform(-150, 150, size=num_rows),\n",
    "    'ball_position_x': np.random.uniform(0.0, 2.0, size=num_rows),\n",
    "    'ball_position_y': np.random.uniform(0.0, 300.0, size=num_rows),\n",
    "    'ball_position_z': np.random.uniform(0.0, 10.0, size=num_rows)\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data).sort_values(by='timestamp')\n",
    "\n",
    "field = MiLBField()\n",
    "\n",
    "# it shows white background with green triangle \n",
    "#fig, ax = plt.subplots(1, 1)  # get figure before drawing\n",
    "#field.draw(display_range='full', ax=ax)\n",
    "\n",
    "# it shows green background with green triangle \n",
    "field.draw(display_range='full')  # without ax=\n",
    "fig = plt.gcf()  # get figure after drawing\n",
    "\n",
    "def update(frame):\n",
    "    print(f'frame: {frame:.2f}')\n",
    "\n",
    "    frame_data = df[ df['timestamp'] <= frame ]\n",
    "    #frame_data = df[ df['timestamp'] <= frame ].drop_duplicates(subset=['player_position'], keep='last')\n",
    "    print('len(frame_data):', len(frame_data))\n",
    "    \n",
    "    players = frame_data  # no need [['field_x', 'field_y']]\n",
    "    balls   = frame_data  # no need [['ball_position_x', 'ball_position_y']]\n",
    "    #players = frame_data.drop_duplicates(subset=['player_position'], keep='last')\n",
    "    print('len(players), len(balls):', len(players), len(balls))\n",
    "\n",
    "    players_colors = players['player_position']\n",
    "    balls_colors   = ['red'] * len(balls)\n",
    "    \n",
    "    p = field.scatter(players['field_x'], players['field_y'], c=players_colors)\n",
    "    b = field.scatter(balls['ball_position_x'], balls['ball_position_y'], c=balls_colors)\n",
    "    \n",
    "    return p, b\n",
    "\n",
    "ani = FuncAnimation(fig, update, frames=np.linspace(df['timestamp'].min(), df['timestamp'].max(), num=10), blit=True)\n",
    "\n",
    "ani.save('animation.gif', writer='imagemagick', fps=2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.3.58 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.55 ðŸš€ Python-3.12.3 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7940MiB)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolo11n.pt, data=data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train2, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=True, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=True, opset=None, workspace=None, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, copy_paste_mode=flip, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train2\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      6640  ultralytics.nn.modules.block.C3k2            [32, 64, 1, False, 0.25]      \n",
      "  3                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      "  4                  -1  1     26080  ultralytics.nn.modules.block.C3k2            [64, 128, 1, False, 0.25]     \n",
      "  5                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      "  6                  -1  1     87040  ultralytics.nn.modules.block.C3k2            [128, 128, 1, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    346112  ultralytics.nn.modules.block.C3k2            [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1    249728  ultralytics.nn.modules.block.C2PSA           [256, 256, 1]                 \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  1    111296  ultralytics.nn.modules.block.C3k2            [384, 128, 1, False]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  1     32096  ultralytics.nn.modules.block.C3k2            [256, 64, 1, False]           \n",
      " 17                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  1     86720  ultralytics.nn.modules.block.C3k2            [192, 128, 1, False]          \n",
      " 20                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  1    378880  ultralytics.nn.modules.block.C3k2            [384, 256, 1, True]           \n",
      " 23        [16, 19, 22]  1    430867  ultralytics.nn.modules.head.Detect           [1, [64, 128, 256]]           \n",
      "YOLO11n summary: 319 layers, 2,590,035 parameters, 2,590,019 gradients, 6.4 GFLOPs\n",
      "\n",
      "Transferred 448/499 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed âœ…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/nova/Downloads/text.v1i.yolov11/train/labels... 131 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 131/131 [00:00<00:00, 1930.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /home/nova/Downloads/text.v1i.yolov11/train/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/nova/Downloads/text.v1i.yolov11/valid/labels... 4 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4/4 [00:00<00:00, 831.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /home/nova/Downloads/text.v1i.yolov11/valid/labels.cache\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to runs/detect/train2/labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.002, momentum=0.9) with parameter groups 81 weight(decay=0.0), 88 weight(decay=0.0005), 87 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/detect/train2\u001b[0m\n",
      "Starting training for 50 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/50      2.49G     0.7688      3.431      1.009          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  5.70it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4    0.00333          1      0.693      0.603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       2/50      2.47G     0.7393      2.556     0.9444          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.55it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 43.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4    0.00333          1      0.845      0.672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       3/50      2.49G     0.7578      1.872      0.896          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.642          1      0.995      0.765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       4/50      2.48G     0.7198      1.658     0.9203          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.98it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.579        0.5      0.788      0.481\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       5/50      2.49G     0.8284      1.837     0.9635          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 42.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.973          1      0.995      0.799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       6/50      2.49G     0.8014      1.677     0.9833          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.14it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.184      0.455      0.249       0.17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       7/50      2.49G     0.8115      1.609     0.9555          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.28it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 42.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.563      0.657      0.745      0.583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       8/50      2.49G     0.8482      3.278     0.9862          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.31it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.967          1      0.995       0.69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       9/50      2.49G     0.6791      1.367     0.9077          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.06it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 42.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.181       0.25      0.193      0.133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10/50      2.49G     0.6985      1.483      0.976          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.38it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 44.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1      0.745      0.945      0.731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      11/50      2.49G     0.7947       1.49      1.008          2        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.33it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 42.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.782          1      0.895      0.598\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      12/50      2.49G     0.7315      1.285     0.9448          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.98          1      0.995      0.738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      13/50      2.48G       0.63      1.075     0.9145          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.739      0.718      0.691      0.473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      14/50      2.49G     0.6665      1.187      0.917          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.13it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.736       0.75      0.808      0.589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      15/50      2.49G     0.6544      1.168     0.9245          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.16it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.64          1      0.828      0.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/50      2.49G     0.5502      1.052     0.8763          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.74it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 29.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1      0.954      0.995       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      17/50      2.48G     0.5862      0.914     0.8693          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 23.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.982          1      0.995      0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      18/50      2.49G     0.5923     0.9853     0.9055          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.979          1      0.995      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      19/50      2.49G     0.6351     0.9943     0.9163          7        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.93it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.992          1      0.995       0.79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      20/50      2.49G      0.593     0.9424     0.9203          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.86it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1      0.708      0.912      0.762\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      21/50      2.48G     0.5514     0.8375     0.8834          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1       0.75      0.804      0.644\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      22/50      2.49G     0.4944     0.9572     0.8812          1        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.09it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.918       0.75      0.888      0.628\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      23/50      2.49G     0.6054     0.8471     0.9047          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.84it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.78          1      0.945      0.761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      24/50      2.49G     0.5971     0.8553     0.9137          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1      0.976      0.995      0.899\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      25/50      2.48G     0.5032      0.811     0.8613          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.24it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 28.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.946       0.75      0.912      0.749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      26/50      2.49G     0.4903     0.7627     0.8863          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.792          1      0.995      0.821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      27/50      2.49G     0.5001     0.7149     0.8801          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.987          1      0.995      0.814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      28/50      2.49G     0.5014     0.6623     0.8618          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.81it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 34.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.99          1      0.995      0.827\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      29/50      2.48G     0.5138     0.6395     0.8645          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.05it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.994          1      0.995      0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      30/50      2.49G     0.4811     0.6425     0.8662          5        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.79it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 39.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.833\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      31/50      2.49G     0.4837     0.6628     0.8533          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.90it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4          1      0.998      0.995       0.87\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      32/50      2.49G     0.4726     0.6491     0.8854          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.72it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.987          1      0.995      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      33/50      2.48G     0.6442     0.8033     0.9034          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.95it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.99          1      0.995      0.877\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      34/50      2.49G     0.4597      0.592     0.8887          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 38.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.974          1      0.995      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      35/50      2.49G      0.498     0.6156     0.8871          4        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      36/50      2.49G     0.4327     0.5646     0.8579          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.60it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.979          1      0.995       0.87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      37/50      2.48G       0.43     0.5374     0.8383          8        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.00it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.833\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      38/50      2.49G     0.4163     0.5358      0.864          6        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.77it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.877\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      39/50      2.49G     0.4167     0.5357     0.8715          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 25.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4       0.99          1      0.995      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      40/50      2.49G     0.3847     0.5411     0.8525          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.89it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 32.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      41/50      2.48G     0.3653     0.5979     0.8753          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:01<00:00,  6.88it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 35.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      42/50      2.49G     0.3701     0.6245     0.8543          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.97it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.987          1      0.995      0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      43/50      2.49G     0.3981     0.6573     0.8647          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.01it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      44/50      2.49G     0.3782     0.5862     0.8669          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.83it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 36.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      45/50      2.48G      0.342     0.5617     0.8836          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.15it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 37.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.852\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      46/50      2.49G     0.3669      0.561     0.8894          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.87it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 33.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      47/50      2.49G     0.3242     0.5441     0.8466          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.02it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.988          1      0.995      0.883\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      48/50      2.49G     0.4024      0.604     0.8937          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00,  9.99it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 40.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      49/50      2.48G      0.351     0.5616      0.844          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.26it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 46.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      50/50      2.49G     0.3122     0.5469     0.8413          3        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9/9 [00:00<00:00, 10.04it/s]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00, 41.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.989          1      0.995      0.895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "50 epochs completed in 0.018 hours.\n",
      "Optimizer stripped from runs/detect/train2/weights/last.pt, 5.5MB\n",
      "Optimizer stripped from runs/detect/train2/weights/best.pt, 5.5MB\n",
      "\n",
      "Validating runs/detect/train2/weights/best.pt...\n",
      "Ultralytics 8.3.55 ðŸš€ Python-3.12.3 torch-2.5.1+cu124 CUDA:0 (NVIDIA GeForce RTX 4060 Laptop GPU, 7940MiB)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,347 parameters, 0 gradients, 6.3 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:00<00:00,  5.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all          4          4      0.986          1      0.995      0.827\n",
      "Speed: 0.3ms preprocess, 38.5ms inference, 0.0ms loss, 1.1ms postprocess per image\n",
      "Results saved to \u001b[1mruns/detect/train2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "results = model.train(data=\"data.yaml\", epochs=50, imgsz=640, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;31merror\u001b[0m: \u001b[1mexternally-managed-environment\u001b[0m\n",
      "\n",
      "\u001b[31mÃ—\u001b[0m This environment is externally managed\n",
      "\u001b[31mâ•°â”€>\u001b[0m To install Python packages system-wide, try apt install\n",
      "\u001b[31m   \u001b[0m python3-xyz, where xyz is the package you are trying to\n",
      "\u001b[31m   \u001b[0m install.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian-packaged Python package,\n",
      "\u001b[31m   \u001b[0m create a virtual environment using python3 -m venv path/to/venv.\n",
      "\u001b[31m   \u001b[0m Then use path/to/venv/bin/python and path/to/venv/bin/pip. Make\n",
      "\u001b[31m   \u001b[0m sure you have python3-full installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m If you wish to install a non-Debian packaged Python application,\n",
      "\u001b[31m   \u001b[0m it may be easiest to use pipx install xyz, which will manage a\n",
      "\u001b[31m   \u001b[0m virtual environment for you. Make sure you have pipx installed.\n",
      "\u001b[31m   \u001b[0m \n",
      "\u001b[31m   \u001b[0m See /usr/share/doc/python3.12/README.venv for more information.\n",
      "\n",
      "\u001b[1;35mnote\u001b[0m: If you believe this is a mistake, please contact your Python installation or OS distribution provider. You can override this, at the risk of breaking your Python installation or OS, by passing --break-system-packages.\n",
      "\u001b[1;36mhint\u001b[0m: See PEP 668 for the detailed specification.\n"
     ]
    }
   ],
   "source": [
    "!pip install pytesseract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 text, 38.8ms\n",
      "Speed: 1.3ms preprocess, 38.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.7ms\n",
      "Speed: 1.1ms preprocess, 6.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.7ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/nova/.local/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.5ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.7ms\n",
      "Speed: 2.2ms preprocess, 6.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.4ms preprocess, 4.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.5ms\n",
      "Speed: 1.8ms preprocess, 6.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.7ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.3ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.5ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.7ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.4ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.2ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.3ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.6ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.6ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.3ms\n",
      "Speed: 1.4ms preprocess, 5.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.4ms\n",
      "Speed: 1.1ms preprocess, 5.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.5ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.8ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.3ms\n",
      "Speed: 1.8ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.7ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.3ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.4ms preprocess, 5.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.8ms preprocess, 5.5ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.2ms preprocess, 5.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.5ms preprocess, 4.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.5ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.0ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.2ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.6ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.4ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.0ms\n",
      "Speed: 1.7ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.4ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.6ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 5.5ms\n",
      "Speed: 1.9ms preprocess, 5.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.1ms\n",
      "Speed: 1.4ms preprocess, 6.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 5 texts, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.8ms\n",
      "Speed: 1.6ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.5ms\n",
      "Speed: 1.4ms preprocess, 5.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.0ms\n",
      "Speed: 1.2ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.8ms\n",
      "Speed: 1.2ms preprocess, 4.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.6ms\n",
      "Speed: 1.5ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 6.4ms\n",
      "Speed: 1.8ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.4ms preprocess, 4.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.5ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.3ms\n",
      "Speed: 1.6ms preprocess, 5.3ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.3ms\n",
      "Speed: 1.1ms preprocess, 5.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.1ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.6ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.3ms\n",
      "Speed: 1.7ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.4ms\n",
      "Speed: 1.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 5.8ms\n",
      "Speed: 2.1ms preprocess, 5.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 5.3ms\n",
      "Speed: 1.3ms preprocess, 5.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.7ms\n",
      "Speed: 1.0ms preprocess, 4.7ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 5.2ms\n",
      "Speed: 1.1ms preprocess, 5.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 5.8ms\n",
      "Speed: 1.5ms preprocess, 5.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.7ms preprocess, 4.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.4ms\n",
      "Speed: 1.4ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.3ms\n",
      "Speed: 1.5ms preprocess, 5.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.4ms preprocess, 4.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 7.9ms\n",
      "Speed: 1.2ms preprocess, 7.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 6.8ms\n",
      "Speed: 1.3ms preprocess, 6.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.4ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 5.0ms\n",
      "Speed: 1.2ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.0ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.1ms preprocess, 4.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.5ms\n",
      "Speed: 1.5ms preprocess, 4.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.4ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.5ms\n",
      "Speed: 1.4ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.2ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.6ms\n",
      "Speed: 1.3ms preprocess, 4.6ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.4ms preprocess, 4.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 5.4ms\n",
      "Speed: 1.3ms preprocess, 5.4ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.0ms preprocess, 4.5ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.4ms preprocess, 5.0ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 (no detections), 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 0.9ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('/home/nova/Downloads/text.v1i.yolov11/runs/detect/train2/weights/text.pt')  # load a pretrained YOLO segmentation model\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"/home/nova/Videos/try.mp4\")\n",
    "\n",
    "# Get the width, height, and FPS of the video\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Set up video writer to save output\n",
    "writer = cv2.VideoWriter(\"Ultralytics_circle_annotation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "# Loop through the video frames\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Create an Annotator instance for the current frame\n",
    "    annotator = Annotator(im0)\n",
    "\n",
    "    # Perform detection\n",
    "    results = model.predict(im0)\n",
    "\n",
    "    # Get the detected bounding boxes and classes\n",
    "    boxes = results[0].boxes.xyxy.tolist()  # Get bounding boxes as a list\n",
    "    clss = results[0].boxes.cls.tolist()  # Get classes as a list\n",
    "\n",
    "    # Annotate the image with circles and labels\n",
    "    for box, cls in zip(boxes, clss):\n",
    "        annotator.circle_label(box, label=str(int(cls)))  # Using str() for the label\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    writer.write(im0)\n",
    "\n",
    "    # Show the annotated frame\n",
    "    cv2.imshow(\"Ultralytics Circle Annotation\", im0)\n",
    "\n",
    "    # Exit the loop if the user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources after processing\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 text, 41.5ms\n",
      "Speed: 2.1ms preprocess, 41.5ms inference, 313.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 2.0ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "qt.qpa.plugin: Could not find the Qt platform plugin \"wayland\" in \"/home/nova/.local/lib/python3.12/site-packages/cv2/qt/plugins\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "0: 384x640 1 text, 5.6ms\n",
      "Speed: 1.5ms preprocess, 5.6ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.1ms preprocess, 5.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.8ms preprocess, 4.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.3ms preprocess, 3.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.0ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.5ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.8ms\n",
      "Speed: 1.9ms preprocess, 3.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.7ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.1ms preprocess, 4.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.8ms\n",
      "Speed: 1.3ms preprocess, 5.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.5ms preprocess, 5.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.3ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.9ms\n",
      "Speed: 1.7ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.7ms preprocess, 4.9ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.2ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.3ms preprocess, 4.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.1ms preprocess, 4.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.3ms preprocess, 4.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.8ms\n",
      "Speed: 1.2ms preprocess, 3.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.0ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.1ms preprocess, 4.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.1ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 0.9ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.2ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.2ms preprocess, 4.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.6ms\n",
      "Speed: 1.3ms preprocess, 5.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.8ms\n",
      "Speed: 1.0ms preprocess, 4.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.5ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.8ms\n",
      "Speed: 1.0ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.8ms\n",
      "Speed: 1.1ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.9ms\n",
      "Speed: 1.2ms preprocess, 4.9ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 7.8ms\n",
      "Speed: 1.0ms preprocess, 7.8ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.0ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 8.9ms\n",
      "Speed: 1.3ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.3ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.5ms\n",
      "Speed: 1.3ms preprocess, 4.5ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.0ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.7ms\n",
      "Speed: 1.1ms preprocess, 3.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.0ms\n",
      "Speed: 1.3ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 0.9ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 5.1ms\n",
      "Speed: 1.3ms preprocess, 5.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.4ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.8ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.3ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.1ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.2ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.8ms\n",
      "Speed: 0.9ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.1ms preprocess, 4.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 6.0ms\n",
      "Speed: 1.1ms preprocess, 6.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.2ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.0ms\n",
      "Speed: 1.1ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.7ms\n",
      "Speed: 1.1ms preprocess, 3.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 6.4ms\n",
      "Speed: 1.6ms preprocess, 6.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.6ms\n",
      "Speed: 1.2ms preprocess, 4.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 8.8ms\n",
      "Speed: 1.4ms preprocess, 8.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.7ms\n",
      "Speed: 1.2ms preprocess, 4.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 0.9ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.0ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.6ms\n",
      "Speed: 1.0ms preprocess, 3.6ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.1ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 3.7ms\n",
      "Speed: 1.0ms preprocess, 3.7ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.1ms\n",
      "Speed: 1.0ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.9ms\n",
      "Speed: 1.3ms preprocess, 4.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 3.8ms\n",
      "Speed: 1.1ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 4 texts, 4.2ms\n",
      "Speed: 1.2ms preprocess, 4.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 5.0ms\n",
      "Speed: 1.6ms preprocess, 5.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.0ms\n",
      "Speed: 1.2ms preprocess, 4.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.0ms preprocess, 4.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 3.8ms\n",
      "Speed: 1.0ms preprocess, 3.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 3.9ms\n",
      "Speed: 1.1ms preprocess, 3.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.0ms\n",
      "Speed: 1.3ms preprocess, 4.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 5.0ms\n",
      "Speed: 1.8ms preprocess, 5.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.2ms\n",
      "Speed: 1.5ms preprocess, 4.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 2 texts, 4.1ms\n",
      "Speed: 1.1ms preprocess, 4.1ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.3ms\n",
      "Speed: 1.2ms preprocess, 4.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 3.9ms\n",
      "Speed: 1.0ms preprocess, 3.9ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.4ms\n",
      "Speed: 1.2ms preprocess, 4.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 text, 4.1ms\n",
      "Speed: 1.6ms preprocess, 4.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 3 texts, 4.1ms\n",
      "Speed: 1.2ms preprocess, 4.1ms inference, 0.9ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "import pytesseract\n",
    "\n",
    "# Load the YOLO model\n",
    "model = YOLO('/home/nova/Downloads/text.v1i.yolov11/runs/detect/train2/weights/best.pt')  # load a pretrained YOLO segmentation model\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"/home/nova/Videos/try.mp4\")\n",
    "\n",
    "# Get the width, height, and FPS of the video\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Set up video writer to save output\n",
    "writer = cv2.VideoWriter(\"Ultralytics_circle_annotation.avi\", cv2.VideoWriter_fourcc(*\"MJPG\"), fps, (w, h))\n",
    "\n",
    "# Loop through the video frames\n",
    "while True:\n",
    "    ret, im0 = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Create an Annotator instance for the current frame\n",
    "    annotator = Annotator(im0)\n",
    "\n",
    "    # Perform detection\n",
    "    results = model.predict(im0)\n",
    "\n",
    "    # Get the detected bounding boxes and classes\n",
    "    boxes = results[0].boxes.xyxy.tolist()  # Get bounding boxes as a list\n",
    "    clss = results[0].boxes.cls.tolist()  # Get classes as a list\n",
    "\n",
    "    # Annotate the image with circles and labels\n",
    "    for box, cls in zip(boxes, clss):\n",
    "        annotator.circle_label(box, label=str(int(cls)))  # Using str() for the label\n",
    "\n",
    "        # Crop the image area inside the bounding box to apply OCR\n",
    "        x1, y1, x2, y2 = map(int, box)  # Convert to integer\n",
    "        cropped_img = im0[y1:y2, x1:x2]  # Crop the region from the original image\n",
    "\n",
    "        # Use Tesseract to extract text from the cropped image\n",
    "        extracted_text = pytesseract.image_to_string(cropped_img)\n",
    "\n",
    "        # Display the extracted text in the image (optional)\n",
    "        cv2.putText(im0, extracted_text, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 1, cv2.LINE_AA)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    writer.write(im0)\n",
    "\n",
    "    # Show the annotated frame\n",
    "    cv2.imshow(\"Ultralytics Circle Annotation\", im0)\n",
    "\n",
    "    # Exit the loop if the user presses 'q'\n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "# Release resources after processing\n",
    "writer.release()\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'super_gradients'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpathlib\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuper_gradients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtraining\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m models\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msuper_gradients\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcommon\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mobject_names\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'super_gradients'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import pathlib\n",
    "from super_gradients.training import models\n",
    "from super_gradients.common.object_names import Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement onnxruntime==1.15.0 (from super-gradients) (from versions: 1.17.0, 1.17.1, 1.17.3, 1.18.0, 1.18.1, 1.19.0, 1.19.2, 1.20.0, 1.20.1)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for onnxruntime==1.15.0\u001b[0m\u001b[31m\n",
      "\u001b[0m^C\n",
      "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -qq super-gradients==3.7.1\n",
    "!pip install -qq tensorrt~=8.6 pycuda pytorch-quantization==2.1.2 --extra-index-url https://pypi.ngc.nvidia.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting super_gradients\n",
      "  Using cached super_gradients-3.7.1-py3-none-any.whl.metadata (41 kB)\n",
      "Collecting torch>=1.9.0 (from super_gradients)\n",
      "  Using cached torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl.metadata (28 kB)\n",
      "Collecting tqdm>=4.57.0 (from super_gradients)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting boto3>=1.17.15 (from super_gradients)\n",
      "  Using cached boto3-1.35.96-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting jsonschema>=3.2.0 (from super_gradients)\n",
      "  Using cached jsonschema-4.23.0-py3-none-any.whl.metadata (7.9 kB)\n",
      "Collecting Deprecated>=1.2.11 (from super_gradients)\n",
      "  Using cached Deprecated-1.2.15-py2.py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting scipy>=1.6.1 (from super_gradients)\n",
      "  Using cached scipy-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "Collecting matplotlib>=3.3.4 (from super_gradients)\n",
      "  Using cached matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from super_gradients) (6.1.1)\n",
      "Collecting tensorboard>=2.4.1 (from super_gradients)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: setuptools>=65.5.1 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from super_gradients) (75.1.0)\n",
      "Collecting coverage~=5.3.1 (from super_gradients)\n",
      "  Using cached coverage-5.3.1.tar.gz (684 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting torchvision>=0.10.0 (from super_gradients)\n",
      "  Using cached torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl.metadata (6.1 kB)\n",
      "Collecting sphinx~=4.0.2 (from super_gradients)\n",
      "  Using cached Sphinx-4.0.3-py3-none-any.whl.metadata (8.0 kB)\n",
      "Collecting sphinx-rtd-theme (from super_gradients)\n",
      "  Using cached sphinx_rtd_theme-3.0.2-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting torchmetrics==0.8 (from super_gradients)\n",
      "  Using cached torchmetrics-0.8.0-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting hydra-core>=1.2.0 (from super_gradients)\n",
      "  Using cached hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
      "Collecting omegaconf (from super_gradients)\n",
      "  Using cached omegaconf-2.3.0-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting onnxruntime==1.15.0 (from super_gradients)\n",
      "  Using cached onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting onnx==1.15.0 (from super_gradients)\n",
      "  Using cached onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting pillow>=10.2.0 (from super_gradients)\n",
      "  Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (9.1 kB)\n",
      "Collecting pip-tools>=6.12.1 (from super_gradients)\n",
      "  Using cached pip_tools-7.4.1-py3-none-any.whl.metadata (26 kB)\n",
      "Collecting einops==0.3.2 (from super_gradients)\n",
      "  Using cached einops-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting protobuf==3.20.3 (from super_gradients)\n",
      "  Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (679 bytes)\n",
      "Collecting treelib==1.6.1 (from super_gradients)\n",
      "  Using cached treelib-1.6.1.tar.gz (24 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting termcolor==1.1.0 (from super_gradients)\n",
      "  Using cached termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: packaging>=20.4 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from super_gradients) (24.2)\n",
      "Requirement already satisfied: wheel>=0.38.0 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from super_gradients) (0.44.0)\n",
      "Requirement already satisfied: pygments>=2.7.4 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from super_gradients) (2.19.1)\n",
      "Collecting stringcase>=1.2.0 (from super_gradients)\n",
      "  Using cached stringcase-1.2.0.tar.gz (3.0 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting numpy<=1.23 (from super_gradients)\n",
      "  Using cached numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.2 kB)\n",
      "Collecting rapidfuzz (from super_gradients)\n",
      "  Using cached rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Collecting json-tricks==3.16.1 (from super_gradients)\n",
      "  Using cached json_tricks-3.16.1-py2.py3-none-any.whl.metadata (17 kB)\n",
      "Collecting onnxsim<1.0,>=0.4.3 (from super_gradients)\n",
      "  Using cached onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
      "Collecting data-gradients~=0.3.1 (from super_gradients)\n",
      "  Using cached data_gradients-0.3.2-py3-none-any.whl.metadata (21 kB)\n",
      "Collecting albumentations~=1.3 (from super_gradients)\n",
      "  Using cached albumentations-1.4.24-py3-none-any.whl.metadata (37 kB)\n",
      "Collecting fonttools>=4.43.0 (from super_gradients)\n",
      "  Using cached fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (165 kB)\n",
      "Collecting werkzeug>=2.3.8 (from super_gradients)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting coloredlogs (from onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Collecting flatbuffers (from onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached flatbuffers-24.12.23-py2.py3-none-any.whl.metadata (876 bytes)\n",
      "Collecting sympy (from onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached sympy-1.13.3-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting pyDeprecate==0.3.* (from torchmetrics==0.8->super_gradients)\n",
      "  Using cached pyDeprecate-0.3.2-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting future (from treelib==1.6.1->super_gradients)\n",
      "  Using cached future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "INFO: pip is looking at multiple versions of albumentations to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting albumentations~=1.3 (from super_gradients)\n",
      "  Using cached albumentations-1.4.23-py3-none-any.whl.metadata (36 kB)\n",
      "  Using cached albumentations-1.4.22-py3-none-any.whl.metadata (33 kB)\n",
      "  Using cached albumentations-1.4.21-py3-none-any.whl.metadata (31 kB)\n",
      "  Using cached albumentations-1.4.20-py3-none-any.whl.metadata (32 kB)\n",
      "  Using cached albumentations-1.4.19-py3-none-any.whl.metadata (32 kB)\n",
      "  Using cached albumentations-1.4.18-py3-none-any.whl.metadata (32 kB)\n",
      "  Using cached albumentations-1.4.17-py3-none-any.whl.metadata (38 kB)\n",
      "INFO: pip is still looking at multiple versions of albumentations to determine which version is compatible with other requirements. This could take a while.\n",
      "  Using cached albumentations-1.4.16-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached albumentations-1.4.15-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached albumentations-1.4.14-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached albumentations-1.4.13-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached albumentations-1.4.12-py3-none-any.whl.metadata (38 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Using cached albumentations-1.4.11-py3-none-any.whl.metadata (39 kB)\n",
      "  Using cached albumentations-1.4.10-py3-none-any.whl.metadata (38 kB)\n",
      "  Using cached albumentations-1.4.8-py3-none-any.whl.metadata (37 kB)\n",
      "  Using cached albumentations-1.4.7-py3-none-any.whl.metadata (37 kB)\n",
      "  Using cached albumentations-1.4.6-py3-none-any.whl.metadata (37 kB)\n",
      "  Using cached albumentations-1.4.4-py3-none-any.whl.metadata (37 kB)\n",
      "  Using cached albumentations-1.4.3-py3-none-any.whl.metadata (37 kB)\n",
      "  Using cached albumentations-1.4.2-py3-none-any.whl.metadata (36 kB)\n",
      "  Using cached albumentations-1.4.1-py3-none-any.whl.metadata (36 kB)\n",
      "  Using cached albumentations-1.4.0-py3-none-any.whl.metadata (35 kB)\n",
      "  Using cached albumentations-1.3.1-py3-none-any.whl.metadata (34 kB)\n",
      "Collecting scikit-image>=0.16.1 (from albumentations~=1.3->super_gradients)\n",
      "  Using cached scikit_image-0.25.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting PyYAML (from albumentations~=1.3->super_gradients)\n",
      "  Using cached PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
      "Collecting qudida>=0.0.4 (from albumentations~=1.3->super_gradients)\n",
      "  Using cached qudida-0.0.4-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting opencv-python-headless>=4.1.1 (from albumentations~=1.3->super_gradients)\n",
      "  Using cached opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting botocore<1.36.0,>=1.35.96 (from boto3>=1.17.15->super_gradients)\n",
      "  Using cached botocore-1.35.96-py3-none-any.whl.metadata (5.7 kB)\n",
      "Collecting jmespath<2.0.0,>=0.7.1 (from boto3>=1.17.15->super_gradients)\n",
      "  Using cached jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3>=1.17.15->super_gradients)\n",
      "  Using cached s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: platformdirs>=2.5.2 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from data-gradients~=0.3.1->super_gradients) (4.3.6)\n",
      "Collecting opencv-python (from data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Collecting seaborn (from data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached seaborn-0.13.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting xhtml2pdf==0.2.11 (from data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached xhtml2pdf-0.2.11.tar.gz (108 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting jinja2 (from data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached jinja2-3.1.5-py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting imagededup (from data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached imagededup-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.5 kB)\n",
      "Collecting arabic-reshaper>=3.0.0 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached arabic_reshaper-3.0.0-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting html5lib>=1.0.1 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached html5lib-1.1-py2.py3-none-any.whl.metadata (16 kB)\n",
      "Collecting pyHanko>=0.12.1 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pyHanko-0.25.3-py3-none-any.whl.metadata (9.4 kB)\n",
      "Collecting pyhanko-certvalidator>=0.19.5 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pyhanko_certvalidator-0.26.5-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting pypdf>=3.1.0 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pypdf-5.1.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Collecting python-bidi>=0.4.2 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.9 kB)\n",
      "Collecting reportlab<4,>=3.5.53 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached reportlab-3.6.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.2 kB)\n",
      "Collecting svglib>=1.2.1 (from xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached svglib-1.5.1.tar.gz (913 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting wrapt<2,>=1.10 (from Deprecated>=1.2.11->super_gradients)\n",
      "  Using cached wrapt-1.17.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.2.0->super_gradients)\n",
      "  Using cached antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting attrs>=22.2.0 (from jsonschema>=3.2.0->super_gradients)\n",
      "  Using cached attrs-24.3.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.2.0->super_gradients)\n",
      "  Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting referencing>=0.28.4 (from jsonschema>=3.2.0->super_gradients)\n",
      "  Using cached referencing-0.35.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting rpds-py>=0.7.1 (from jsonschema>=3.2.0->super_gradients)\n",
      "  Using cached rpds_py-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.2 kB)\n",
      "Collecting contourpy>=1.0.1 (from matplotlib>=3.3.4->super_gradients)\n",
      "  Using cached contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.4 kB)\n",
      "Collecting cycler>=0.10 (from matplotlib>=3.3.4->super_gradients)\n",
      "  Using cached cycler-0.12.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting kiwisolver>=1.3.1 (from matplotlib>=3.3.4->super_gradients)\n",
      "  Using cached kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (6.2 kB)\n",
      "Collecting pyparsing>=2.3.1 (from matplotlib>=3.3.4->super_gradients)\n",
      "  Using cached pyparsing-3.2.1-py3-none-any.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from matplotlib>=3.3.4->super_gradients) (2.9.0.post0)\n",
      "Collecting rich (from onnxsim<1.0,>=0.4.3->super_gradients)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting build>=1.0.0 (from pip-tools>=6.12.1->super_gradients)\n",
      "  Using cached build-1.2.2.post1-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting click>=8 (from pip-tools>=6.12.1->super_gradients)\n",
      "  Using cached click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
      "Requirement already satisfied: pip>=22.2 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from pip-tools>=6.12.1->super_gradients) (24.2)\n",
      "Collecting pyproject-hooks (from pip-tools>=6.12.1->super_gradients)\n",
      "  Using cached pyproject_hooks-1.2.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting tomli (from pip-tools>=6.12.1->super_gradients)\n",
      "  Using cached tomli-2.2.1-py3-none-any.whl.metadata (10 kB)\n",
      "INFO: pip is looking at multiple versions of scipy to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scipy>=1.6.1 (from super_gradients)\n",
      "  Using cached scipy-1.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.14.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "  Using cached scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting sphinxcontrib-applehelp (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_applehelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-devhelp (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_devhelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-jsmath (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting sphinxcontrib-htmlhelp (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting sphinxcontrib-serializinghtml (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting sphinxcontrib-qthelp (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached sphinxcontrib_qthelp-2.0.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting docutils<0.18,>=0.14 (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached docutils-0.17.1-py2.py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting snowballstemmer>=1.1 (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached snowballstemmer-2.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting babel>=1.3 (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached babel-2.16.0-py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting alabaster<0.8,>=0.7 (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached alabaster-0.7.16-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting imagesize (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached imagesize-1.4.1-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting requests>=2.5.0 (from sphinx~=4.0.2->super_gradients)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting absl-py>=0.4 (from tensorboard>=2.4.1->super_gradients)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting grpcio>=1.48.2 (from tensorboard>=2.4.1->super_gradients)\n",
      "  Using cached grpcio-1.69.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.9 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard>=2.4.1->super_gradients)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: six>1.9 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from tensorboard>=2.4.1->super_gradients) (1.17.0)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard>=2.4.1->super_gradients)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting filelock (from torch>=1.9.0->super_gradients)\n",
      "  Using cached filelock-3.16.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/nova/anaconda3/envs/deci/lib/python3.10/site-packages (from torch>=1.9.0->super_gradients) (4.12.2)\n",
      "Collecting networkx (from torch>=1.9.0->super_gradients)\n",
      "  Using cached networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n",
      "Collecting fsspec (from torch>=1.9.0->super_gradients)\n",
      "  Using cached fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
      "Collecting nvidia-nvtx-cu12==12.4.127 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.7 kB)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting triton==3.1.0 (from torch>=1.9.0->super_gradients)\n",
      "  Using cached triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.3 kB)\n",
      "Collecting sympy (from onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached sympy-1.13.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting mpmath<1.4,>=1.1.0 (from sympy->onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached mpmath-1.3.0-py3-none-any.whl.metadata (8.6 kB)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=2.3.8->super_gradients)\n",
      "  Using cached MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "INFO: pip is looking at multiple versions of sphinx-rtd-theme to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting sphinx-rtd-theme (from super_gradients)\n",
      "  Using cached sphinx_rtd_theme-3.0.1-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached sphinx_rtd_theme-3.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached sphinx_rtd_theme-2.0.0-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "  Using cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl.metadata (4.5 kB)\n",
      "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->super_gradients)\n",
      "  Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl.metadata (2.6 kB)\n",
      "Collecting urllib3!=2.2.0,<3,>=1.25.4 (from botocore<1.36.0,>=1.35.96->boto3>=1.17.15->super_gradients)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting scikit-learn>=0.19.1 (from qudida>=0.0.4->albumentations~=1.3->super_gradients)\n",
      "  Using cached scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests>=2.5.0->sphinx~=4.0.2->super_gradients)\n",
      "  Using cached charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests>=2.5.0->sphinx~=4.0.2->super_gradients)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests>=2.5.0->sphinx~=4.0.2->super_gradients)\n",
      "  Using cached certifi-2024.12.14-py3-none-any.whl.metadata (2.3 kB)\n",
      "INFO: pip is looking at multiple versions of scikit-image to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting scikit-image>=0.16.1 (from albumentations~=1.3->super_gradients)\n",
      "  Using cached scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting imageio>=2.33 (from scikit-image>=0.16.1->albumentations~=1.3->super_gradients)\n",
      "  Using cached imageio-2.36.1-py3-none-any.whl.metadata (5.2 kB)\n",
      "Collecting tifffile>=2022.8.12 (from scikit-image>=0.16.1->albumentations~=1.3->super_gradients)\n",
      "  Using cached tifffile-2024.12.12-py3-none-any.whl.metadata (31 kB)\n",
      "Collecting lazy-loader>=0.4 (from scikit-image>=0.16.1->albumentations~=1.3->super_gradients)\n",
      "  Using cached lazy_loader-0.4-py3-none-any.whl.metadata (7.6 kB)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime==1.15.0->super_gradients)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Collecting PyWavelets (from imagededup->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->onnxsim<1.0,>=0.4.3->super_gradients)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pandas>=1.2 (from seaborn->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (89 kB)\n",
      "Collecting webencodings (from html5lib>=1.0.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached webencodings-0.5.1-py2.py3-none-any.whl.metadata (2.1 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->onnxsim<1.0,>=0.4.3->super_gradients)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting pytz>=2020.1 (from pandas>=1.2->seaborn->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas>=1.2->seaborn->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting asn1crypto>=1.5.1 (from pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached asn1crypto-1.5.1-py2.py3-none-any.whl.metadata (13 kB)\n",
      "Collecting qrcode>=7.3.1 (from pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached qrcode-8.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting tzlocal>=4.3 (from pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached tzlocal-5.2-py3-none-any.whl.metadata (7.8 kB)\n",
      "Collecting cryptography>=43.0.3 (from pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl.metadata (5.7 kB)\n",
      "Collecting oscrypto>=1.1.0 (from pyhanko-certvalidator>=0.19.5->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached oscrypto-1.3.0-py2.py3-none-any.whl.metadata (15 kB)\n",
      "Collecting uritools>=3.0.1 (from pyhanko-certvalidator>=0.19.5->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached uritools-4.0.3-py3-none-any.whl.metadata (4.7 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations~=1.3->super_gradients)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations~=1.3->super_gradients)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting lxml (from svglib>=1.2.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tinycss2>=0.6.0 (from svglib>=1.2.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached tinycss2-1.4.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting cssselect2>=0.2.0 (from svglib>=1.2.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached cssselect2-0.7.0-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting cffi>=1.12 (from cryptography>=43.0.3->pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting pycparser (from cffi>=1.12->cryptography>=43.0.3->pyHanko>=0.12.1->xhtml2pdf==0.2.11->data-gradients~=0.3.1->super_gradients)\n",
      "  Using cached pycparser-2.22-py3-none-any.whl.metadata (943 bytes)\n",
      "Using cached super_gradients-3.7.1-py3-none-any.whl (12.1 MB)\n",
      "Using cached einops-0.3.2-py3-none-any.whl (25 kB)\n",
      "Using cached json_tricks-3.16.1-py2.py3-none-any.whl (27 kB)\n",
      "Using cached onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "Using cached onnxruntime-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.9 MB)\n",
      "Using cached protobuf-3.20.3-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
      "Using cached torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
      "Using cached pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
      "Using cached albumentations-1.3.1-py3-none-any.whl (125 kB)\n",
      "Using cached boto3-1.35.96-py3-none-any.whl (139 kB)\n",
      "Using cached data_gradients-0.3.2-py3-none-any.whl (459 kB)\n",
      "Using cached Deprecated-1.2.15-py2.py3-none-any.whl (9.9 kB)\n",
      "Using cached fonttools-4.55.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "Using cached hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
      "Using cached jsonschema-4.23.0-py3-none-any.whl (88 kB)\n",
      "Using cached matplotlib-3.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.6 MB)\n",
      "Using cached numpy-1.23.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
      "Using cached omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
      "Using cached onnxsim-0.4.36-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
      "Using cached pillow-11.1.0-cp310-cp310-manylinux_2_28_x86_64.whl (4.5 MB)\n",
      "Using cached pip_tools-7.4.1-py3-none-any.whl (61 kB)\n",
      "Downloading scipy-1.13.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached Sphinx-4.0.3-py3-none-any.whl (2.9 MB)\n",
      "Downloading tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m983.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading torch-2.5.1-cp310-cp310-manylinux1_x86_64.whl (906.4 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m906.4/906.4 MB\u001b[0m \u001b[31m671.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:17\u001b[0mm\n",
      "\u001b[?25hUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "Using cached nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
      "Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "Using cached nvidia_nvtx_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (99 kB)\n",
      "Using cached sympy-1.13.1-py3-none-any.whl (6.2 MB)\n",
      "Downloading triton-3.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.5/209.5 MB\u001b[0m \u001b[31m937.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:06\u001b[0m\n",
      "\u001b[?25hDownloading torchvision-0.20.1-cp310-cp310-manylinux1_x86_64.whl (7.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Downloading rapidfuzz-3.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
      "Downloading absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Downloading alabaster-0.7.16-py3-none-any.whl (13 kB)\n",
      "Using cached attrs-24.3.0-py3-none-any.whl (63 kB)\n",
      "Using cached babel-2.16.0-py3-none-any.whl (9.6 MB)\n",
      "Downloading botocore-1.35.96-py3-none-any.whl (13.3 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m991.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached build-1.2.2.post1-py3-none-any.whl (22 kB)\n",
      "Downloading click-8.1.8-py3-none-any.whl (98 kB)\n",
      "Downloading contourpy-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (324 kB)\n",
      "Downloading cycler-0.12.1-py3-none-any.whl (8.3 kB)\n",
      "Using cached docutils-0.17.1-py2.py3-none-any.whl (575 kB)\n",
      "Downloading grpcio-1.69.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m789.3 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached jinja2-3.1.5-py3-none-any.whl (134 kB)\n",
      "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
      "Using cached jsonschema_specifications-2024.10.1-py3-none-any.whl (18 kB)\n",
      "Downloading kiwisolver-1.4.8-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m743.9 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Downloading MarkupSafe-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m49.9/49.9 MB\u001b[0m \u001b[31m836.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:02\u001b[0m\n",
      "\u001b[?25hDownloading pyparsing-3.2.1-py3-none-any.whl (107 kB)\n",
      "Downloading PyYAML-6.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (751 kB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m751.2/751.2 kB\u001b[0m \u001b[31m610.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading qudida-0.0.4-py3-none-any.whl (3.5 kB)\n",
      "Using cached referencing-0.35.1-py3-none-any.whl (26 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Downloading rpds_py-0.22.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (381 kB)\n",
      "Using cached s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
      "Downloading scikit_image-0.24.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.9 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m14.9/14.9 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hUsing cached networkx-3.4.2-py3-none-any.whl (1.7 MB)\n",
      "Downloading snowballstemmer-2.2.0-py2.py3-none-any.whl (93 kB)\n",
      "Using cached sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
      "Downloading tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tomli-2.2.1-py3-none-any.whl (14 kB)\n",
      "Downloading wrapt-1.17.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached filelock-3.16.1-py3-none-any.whl (16 kB)\n",
      "Downloading flatbuffers-24.12.23-py2.py3-none-any.whl (30 kB)\n",
      "Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Using cached future-1.0.0-py3-none-any.whl (491 kB)\n",
      "Downloading imagededup-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (176 kB)\n",
      "Downloading imagesize-1.4.1-py2.py3-none-any.whl (8.8 kB)\n",
      "Using cached opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
      "Using cached pyproject_hooks-1.2.0-py3-none-any.whl (10 kB)\n",
      "Downloading rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached seaborn-0.13.2-py3-none-any.whl (294 kB)\n",
      "Downloading sphinxcontrib_applehelp-2.0.0-py3-none-any.whl (119 kB)\n",
      "Downloading sphinxcontrib_devhelp-2.0.0-py3-none-any.whl (82 kB)\n",
      "Downloading sphinxcontrib_htmlhelp-2.1.0-py3-none-any.whl (98 kB)\n",
      "Downloading sphinxcontrib_jsmath-1.0.1-py2.py3-none-any.whl (5.1 kB)\n",
      "Downloading sphinxcontrib_qthelp-2.0.0-py3-none-any.whl (88 kB)\n",
      "Downloading sphinxcontrib_serializinghtml-2.0.0-py3-none-any.whl (92 kB)\n",
      "Downloading arabic_reshaper-3.0.0-py3-none-any.whl (20 kB)\n",
      "Using cached certifi-2024.12.14-py3-none-any.whl (164 kB)\n",
      "Downloading charset_normalizer-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (146 kB)\n",
      "Downloading html5lib-1.1-py2.py3-none-any.whl (112 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Downloading imageio-2.36.1-py3-none-any.whl (315 kB)\n",
      "Downloading lazy_loader-0.4-py3-none-any.whl (12 kB)\n",
      "Downloading markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mpmath-1.3.0-py3-none-any.whl (536 kB)\n",
      "Downloading pandas-2.2.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.1 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.1/13.1 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[?25hDownloading pyHanko-0.25.3-py3-none-any.whl (447 kB)\n",
      "Downloading pyhanko_certvalidator-0.26.5-py3-none-any.whl (109 kB)\n",
      "Downloading pypdf-5.1.0-py3-none-any.whl (297 kB)\n",
      "Downloading python_bidi-0.6.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
      "Downloading reportlab-3.6.13-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m472.7 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading scikit_learn-1.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.5/13.5 MB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading tifffile-2024.12.12-py3-none-any.whl (227 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Downloading pywavelets-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.5 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading asn1crypto-1.5.1-py2.py3-none-any.whl (105 kB)\n",
      "Downloading cryptography-44.0.0-cp39-abi3-manylinux_2_28_x86_64.whl (4.2 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cssselect2-0.7.0-py3-none-any.whl (15 kB)\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Downloading oscrypto-1.3.0-py2.py3-none-any.whl (194 kB)\n",
      "Downloading pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading qrcode-8.0-py3-none-any.whl (45 kB)\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Using cached tinycss2-1.4.0-py3-none-any.whl (26 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Downloading tzlocal-5.2-py3-none-any.whl (17 kB)\n",
      "Downloading uritools-4.0.3-py3-none-any.whl (10 kB)\n",
      "Using cached webencodings-0.5.1-py2.py3-none-any.whl (11 kB)\n",
      "Downloading lxml-5.3.0-cp310-cp310-manylinux_2_28_x86_64.whl (5.0 MB)\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading cffi-1.17.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (446 kB)\n",
      "Using cached pycparser-2.22-py3-none-any.whl (117 kB)\n",
      "Building wheels for collected packages: termcolor, treelib, coverage, xhtml2pdf, antlr4-python3-runtime, stringcase, svglib\n",
      "  Building wheel for termcolor (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4832 sha256=06c23f3c60734a515c70344e889ff497fd8d8fa186fc3fffa3f021bf35408ba4\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/a1/49/46/1b13a65d8da11238af9616b00fdde6d45b0f95d9291bac8452\n",
      "  Building wheel for treelib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for treelib: filename=treelib-1.6.1-py3-none-any.whl size=18369 sha256=cea460f0418858223d7276c3617e556082fba3d88dd25cca0372abd01a75c96f\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/63/72/8b/76569b82bf280a03c4e294c3b29ee2398217186369c427ed4b\n",
      "  Building wheel for coverage (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for coverage: filename=coverage-5.3.1-cp310-cp310-linux_x86_64.whl size=209996 sha256=4141b14ffd440c5248dd61ca4e7ffcbf63125cc8ed43ad079e28769e4b5f7145\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/e2/70/10/313be697f460d6024cfa94b7f0e22ffc1c53aab718fb4f42af\n",
      "  Building wheel for xhtml2pdf (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for xhtml2pdf: filename=xhtml2pdf-0.2.11-py3-none-any.whl size=262973 sha256=5e609098a483878ff730b150a8cf334b4e918b4cc3e1860dfd4bc4daa44dca40\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/72/77/fb/e473c11c4e30a7680bf5b1b7f1d07ef04932184a2f39118e8d\n",
      "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144555 sha256=80a2a5785656a5d16d6ef68d2a5661d792d59b54424f1b8e4d4002b44ebcca3c\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
      "  Building wheel for stringcase (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for stringcase: filename=stringcase-1.2.0-py3-none-any.whl size=3568 sha256=23b9c267298a063b6c6858d32d1a53937bf456288bc03666832ac40a35b926ed\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/31/ba/22/1a2d952a9ce8aa86e42fda41e2c87fdaf20e238c88bf8df013\n",
      "  Building wheel for svglib (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for svglib: filename=svglib-1.5.1-py3-none-any.whl size=30907 sha256=ba454ffc75ac0b2c8ef59e79e1413ec92b2e53e6dbf74093bfc872e2aceafae5\n",
      "  Stored in directory: /home/nova/.cache/pip/wheels/56/9f/90/f37f4b9dbf82987a24ae14f15586e96715cb669a4710b3b85d\n",
      "Successfully built termcolor treelib coverage xhtml2pdf antlr4-python3-runtime stringcase svglib\n",
      "Installing collected packages: webencodings, termcolor, stringcase, snowballstemmer, pytz, python-bidi, mpmath, json-tricks, flatbuffers, einops, asn1crypto, arabic-reshaper, antlr4-python3-runtime, wrapt, urllib3, uritools, tzlocal, tzdata, tqdm, tomli, tinycss2, threadpoolctl, tensorboard-data-server, sympy, sphinxcontrib-serializinghtml, sphinxcontrib-qthelp, sphinxcontrib-jsmath, sphinxcontrib-htmlhelp, sphinxcontrib-devhelp, sphinxcontrib-applehelp, rpds-py, rapidfuzz, qrcode, PyYAML, pyproject-hooks, pypdf, pyparsing, pyDeprecate, pycparser, protobuf, pillow, oscrypto, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, networkx, mdurl, MarkupSafe, markdown, lxml, lazy-loader, kiwisolver, joblib, jmespath, imagesize, idna, humanfriendly, html5lib, grpcio, future, fsspec, fonttools, filelock, docutils, cycler, coverage, click, charset-normalizer, certifi, babel, attrs, alabaster, absl-py, werkzeug, triton, treelib, tifffile, scipy, requests, reportlab, referencing, PyWavelets, pandas, opencv-python-headless, opencv-python, onnx, omegaconf, nvidia-cusparse-cu12, nvidia-cudnn-cu12, markdown-it-py, jinja2, imageio, Deprecated, cssselect2, contourpy, coloredlogs, cffi, build, botocore, tensorboard, svglib, sphinx, scikit-learn, scikit-image, s3transfer, rich, pip-tools, onnxruntime, nvidia-cusolver-cu12, matplotlib, jsonschema-specifications, hydra-core, cryptography, torch, sphinxcontrib-jquery, seaborn, qudida, pyhanko-certvalidator, onnxsim, jsonschema, boto3, torchvision, torchmetrics, sphinx-rtd-theme, pyHanko, albumentations, xhtml2pdf, imagededup, data-gradients, super_gradients\n",
      "Successfully installed Deprecated-1.2.15 MarkupSafe-3.0.2 PyWavelets-1.8.0 PyYAML-6.0.2 absl-py-2.1.0 alabaster-0.7.16 albumentations-1.3.1 antlr4-python3-runtime-4.9.3 arabic-reshaper-3.0.0 asn1crypto-1.5.1 attrs-24.3.0 babel-2.16.0 boto3-1.35.96 botocore-1.35.96 build-1.2.2.post1 certifi-2024.12.14 cffi-1.17.1 charset-normalizer-3.4.1 click-8.1.8 coloredlogs-15.0.1 contourpy-1.3.1 coverage-5.3.1 cryptography-44.0.0 cssselect2-0.7.0 cycler-0.12.1 data-gradients-0.3.2 docutils-0.17.1 einops-0.3.2 filelock-3.16.1 flatbuffers-24.12.23 fonttools-4.55.3 fsspec-2024.12.0 future-1.0.0 grpcio-1.69.0 html5lib-1.1 humanfriendly-10.0 hydra-core-1.3.2 idna-3.10 imagededup-0.3.2 imageio-2.36.1 imagesize-1.4.1 jinja2-3.1.5 jmespath-1.0.1 joblib-1.4.2 json-tricks-3.16.1 jsonschema-4.23.0 jsonschema-specifications-2024.10.1 kiwisolver-1.4.8 lazy-loader-0.4 lxml-5.3.0 markdown-3.7 markdown-it-py-3.0.0 matplotlib-3.10.0 mdurl-0.1.2 mpmath-1.3.0 networkx-3.4.2 numpy-1.23.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.4.127 omegaconf-2.3.0 onnx-1.15.0 onnxruntime-1.15.0 onnxsim-0.4.36 opencv-python-4.10.0.84 opencv-python-headless-4.10.0.84 oscrypto-1.3.0 pandas-2.2.3 pillow-11.1.0 pip-tools-7.4.1 protobuf-3.20.3 pyDeprecate-0.3.2 pyHanko-0.25.3 pycparser-2.22 pyhanko-certvalidator-0.26.5 pyparsing-3.2.1 pypdf-5.1.0 pyproject-hooks-1.2.0 python-bidi-0.6.3 pytz-2024.2 qrcode-8.0 qudida-0.0.4 rapidfuzz-3.11.0 referencing-0.35.1 reportlab-3.6.13 requests-2.32.3 rich-13.9.4 rpds-py-0.22.3 s3transfer-0.10.4 scikit-image-0.24.0 scikit-learn-1.6.0 scipy-1.13.1 seaborn-0.13.2 snowballstemmer-2.2.0 sphinx-4.0.3 sphinx-rtd-theme-1.3.0 sphinxcontrib-applehelp-2.0.0 sphinxcontrib-devhelp-2.0.0 sphinxcontrib-htmlhelp-2.1.0 sphinxcontrib-jquery-4.1 sphinxcontrib-jsmath-1.0.1 sphinxcontrib-qthelp-2.0.0 sphinxcontrib-serializinghtml-2.0.0 stringcase-1.2.0 super_gradients-3.7.1 svglib-1.5.1 sympy-1.13.1 tensorboard-2.18.0 tensorboard-data-server-0.7.2 termcolor-1.1.0 threadpoolctl-3.5.0 tifffile-2024.12.12 tinycss2-1.4.0 tomli-2.2.1 torch-2.5.1 torchmetrics-0.8.0 torchvision-0.20.1 tqdm-4.67.1 treelib-1.6.1 triton-3.1.0 tzdata-2024.2 tzlocal-5.2 uritools-4.0.3 urllib3-2.3.0 webencodings-0.5.1 werkzeug-3.1.3 wrapt-1.17.0 xhtml2pdf-0.2.11\n"
     ]
    }
   ],
   "source": [
    "!pip install super_gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading https://ultralytics.com/images/bus.jpg to 'bus.jpg'...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 134k/134k [00:00<00:00, 4.87MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image 1/1 /home/nova/Downloads/text.v1i.yolov11/bus.jpg: 640x480 4 persons, 136.7ms\n",
      "Speed: 3.0ms preprocess, 136.7ms inference, 1.3ms postprocess per image at shape (1, 3, 640, 480)\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "# Load a model\n",
    "model = YOLO(\"yolo11n-pose.pt\")  # load an official model\n",
    "# Predict with the model\n",
    "results = model(\"https://ultralytics.com/images/bus.jpg\")  # predict on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "usr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
